import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def generate_explanation(risk_context: dict) -> str:
    """
    Generates a prompt for the LLM to explain the detected risk and calls a mock LLM.

    Args:
        risk_context (dict): Context data containing 'uav_id', 'risk_type', 
                             'current_env_wind', 'max_wind'.

    Returns:
        str: The explanation text generated by the (mock) LLM.
    """
    uav_id = risk_context.get("uav_id", "Unknown")
    risk_type = risk_context.get("risk_type", "Unknown Risk")
    env_wind = risk_context.get("current_env_wind", "N/A")
    max_wind = risk_context.get("max_wind", "N/A")

    # 1. Prompt Definition
    prompt = f"""
    角色设定：你是一个低空交通管制 AI。
    输入数据：系统检测到 UAV {uav_id} 存在 {risk_type}。
    推理依据：规则库指出，当环境风速 ({env_wind}级) 超过机型抗风等级 ({max_wind}级) 时，必须报警。
    任务：请生成一段简短、专业的警报文本，并给出建议操作（如：返航、迫降）。
    """
    
    logger.info("Constructed Prompt for LLM:")
    print("-" * 40)
    print(prompt.strip())
    print("-" * 40)

    # 2. Call Mock LLM
    response = mock_llm_response(prompt)
    return response

def mock_llm_response(prompt: str) -> str:
    """
    Simulates a call to a Large Language Model (e.g., GPT-4).
    
    Since API keys are not configured for this demo, we return a 
    high-quality pre-set response to demonstrate the intended output 
    for the paper's Case Study.
    """
    logger.info("Calling Mock LLM Service...")
    
    # Pre-set high quality response
    dummy_response = (
        "【警报：稳定性风险警告】\n"
        "监测对象：SkyMule-5\n"
        "当前状态：环境风速 7级（强风），已超出本机型最大抗风限制（5级）。\n"
        "风险评估：飞行姿态可能失稳，坠机风险极高。\n"
        "管制建议：立即终止当前任务。鉴于风力过大，建议执行紧急迫降程序寻找最近安全平地着陆，切勿强行返航。"
    )
    
    return dummy_response

if __name__ == "__main__":
    # 3. Main Test Case: SkyMule-5 encountering Level 7 wind
    scenario_context = {
        "uav_id": "SkyMule-5",
        "risk_type": "StabilityRisk",
        "current_env_wind": 7,
        "max_wind": 5
    }
    
    print(">>> Starting Risk Explanation Demo...")
    explanation = generate_explanation(scenario_context)
    
    print("\n>>> LLM Agent Response:")
    print(explanation)

